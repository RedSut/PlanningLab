{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Planning-Lab Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/frozen_lake.gif\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to cross a frozen lake to reach the gift in cell $(3,3)$ without falling into any Holes(H) by walking over the Frozen(F) lake. In addition to avoid falling into water holes (cells $(1,1)$, $(1,3)$, $(2,3)$ and $(3,0)$), the agent has to reach its destination as fast as possible, since walking on the frozen surface hurts him.\n",
    "\n",
    "However, the agent may not always move in the chosen direction due to the slippery nature of the frozen lake! In fact, he will move in the chosen direction with probability of 1/2 else it will move in the perpendicular directions with equal probability of 1/4 for each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'F']\n",
      " ['F' 'H' 'F' 'H']\n",
      " ['F' 'F' 'F' 'H']\n",
      " ['H' 'F' 'F' 'G']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of start state:  S\n",
      "Cell type of cell 7:  H\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "# ESEMPI ENV.GRID\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from utils.exam_checker import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'SlipperyFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 7 # a generic state of the environment,\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(f\"Cell type of cell {state}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, and assuming that the discount factor is set to 0.9, find the optimal way for the agent to reach the treasure and print the solution using the provided code: [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15e2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_solution(environment, maxiters=300, discount=0.9, max_error=1e-3): #feel free to add all the arguments you need!\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    U_1 = [0 for _ in range(environment.observation_space.n)] # vector of utilities for states S\n",
    "    delta = 0 # maximum change in the utility o any state in an iteration\n",
    "    U = U_1.copy()\n",
    "\n",
    "    # Code Here!\n",
    "    while True:\n",
    "        delta = 0\n",
    "        U = U_1.copy()\n",
    "\n",
    "        for state in range(environment.observation_space.n): \n",
    "            \n",
    "            prob_vect = []\n",
    "\n",
    "            for a in range(environment.action_space.n):\n",
    "                p = 0\n",
    "                for s in range(environment.observation_space.n):\n",
    "                    p += environment.T[state, a, s] * U[s]\n",
    "                prob_vect.append(p)\n",
    "            \n",
    "            if environment.grid[state] == 'H' or environment.grid[state] == 'G':\n",
    "                U_1[state] = environment.RS[state]\n",
    "            else:\n",
    "                U_1[state] = environment.RS[state] + discount * max(prob_vect)    \n",
    "\n",
    "            if abs(U_1[state]-U[state]) > delta:\n",
    "                delta = abs(U_1[state]-U[state])\n",
    "                \n",
    "        if delta < (max_error * (1 - discount)/discount):\n",
    "            break\n",
    "        \n",
    "    #\n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy\n",
    "    \n",
    "    \n",
    "    # you should return the policy as a 1-d array of action identifiers, where the 𝑖-th action refers to the 𝑖-th state:\n",
    "    #return np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8beb9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0113\n",
      "\n",
      "Environment: SlipperyFrozenLakeEnv-v0\n",
      "\n",
      "Your solution:\n",
      "[['D' 'R' 'D' 'L']\n",
      " ['D' 'D' 'D' 'L']\n",
      " ['R' 'D' 'D' 'D']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "solution = your_solution(env)\n",
    "\n",
    "#print(env.T[env.pos_to_state(0, 1), 0, env.pos_to_state(0, 0)])\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "# this function converts your solution to the corresponding 2-d action matrix:\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "check_1_1(env_name, solution_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b715f9",
   "metadata": {},
   "source": [
    "### 1.2) Now focus on state (2,1) and assume the value funcion is given (see code). Discuss which action is the optimal one for that specific state and why. Use code to motivate your answer: [15 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf924170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct action to perform should be: 3\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [-0.33, -0.36, -0.38, -0.38, -0.3, -1, -0, -1, -0.22, 0.02, -0.03, -1, -1, 0.23, 0.52, 1]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "\n",
    "'''\n",
    "YOUR CODE HERE\n",
    "'''\n",
    "prob_vect = []\n",
    "\n",
    "for a in range(len(actions)):\n",
    "    p = 0\n",
    "    for s in range(len(value_function)):\n",
    "        p += env.T[id_start_state, a, s] * value_function[s]\n",
    "    prob_vect.append(p)\n",
    "\n",
    "import numpy as np\n",
    "correct_action = np.argmax(prob_vect)\n",
    "    \n",
    "print(f'The correct action to perform should be: {correct_action}')\n",
    "check_2_1(correct_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d79d8",
   "metadata": {},
   "source": [
    "### 1.3) Now suppose that we want the agent to avoid holes at all costs, as shown in the policy below:\n",
    "<img src=\"images/policy.png\"/>\n",
    "\n",
    "### Change the reward array in order to force the agent to behave in this specific way. [15 Points]\n",
    "(the start and goal positions are the same as in exercise 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71056459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: ModifiableFrozenLakeEnv-v0\n",
      "\n",
      "Your solution:\n",
      "[['L' 'U' 'U' 'U']\n",
      " ['L' 'L' 'D' 'U']\n",
      " ['U' 'D' 'L' 'D']\n",
      " ['R' 'R' 'R' 'L']]\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env_name = 'ModifiableFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Change the reward specification above, the reward array will be updated by a dedicated funcion:\n",
    "rewards = {\"F\": -0.04, \"S\": -0.04, \"H\": -5, \"G\": 4} \n",
    "env.RS = update_reward_array(rewards)\n",
    "\n",
    "solution = your_solution(env) #Use your solution from exercise 1.1 to test your changes\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "check_1_3(env_name, solution_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f319b3",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce193bb",
   "metadata": {},
   "source": [
    "Now consider the following environment:\n",
    "    <img src=\"images/frozen_lake_big.png\" style=\"zoom: 30%;\"/>\n",
    "    \n",
    "The lake is much bigger, but the actions are no longer stochastic. **S=(0,0)** and **G=(7,7)** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (except when a wall is present) and that each movement has a unitary cost. Answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038b0cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'W' 'F' 'F' 'F' 'F' 'W' 'F']\n",
      " ['F' 'W' 'F' 'W' 'F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'W' 'W' 'F' 'F' 'F' 'F']\n",
      " ['W' 'F' 'F' 'F' 'F' 'W' 'F' 'F']\n",
      " ['F' 'F' 'F' 'W' 'F' 'F' 'F' 'W']\n",
      " ['F' 'W' 'W' 'F' 'W' 'F' 'W' 'F']\n",
      " ['F' 'W' 'F' 'F' 'W' 'F' 'W' 'F']\n",
      " ['F' 'F' 'F' 'W' 'F' 'F' 'F' 'G']]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'BigFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61812b97",
   "metadata": {},
   "source": [
    "### 2.1) First of all, verify whether the Euclidean distance (l2_norm) is an  *admissible* heuristic in this environment. In particular, you should implement a function that checks whether for every state, the admissibility condition is verified. The function should return true if the heuristic is consistent and false otherwise. Keep in mind that every action has unitary cost. [15 Points]\n",
    "\n",
    "Hint: The function below can be used as an oracle to know the real cost of going from a node n to the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fe8e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost from start to goal:  14\n",
      "Cost from position (3,3) to goal:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost from start to goal: \", real_cost_to_goal(env.startstate))\n",
    "print(\"Cost from position (3,3) to goal: \", real_cost_to_goal(env.pos_to_state(3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0917d6",
   "metadata": {},
   "source": [
    "#### From now on consider hole cells as unaccessible for the agent (they are in fact referred to as walls, \"W\", in the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "585957d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_euristic(environment):\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    for s in range(environment.observation_space.n): \n",
    "        if env.grid[s] == \"W\":\n",
    "            continue\n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        '''\n",
    "        for action in range(environment.action_space.n): # Per ogni azione considero lo stato successivo s'\n",
    "            \n",
    "            p1 = environment.state_to_pos(environment.sample(s, action)) # environment.sample(s, action) -> s' -> stato che ottengo dopo che eseguo l'azione a nello stato s\n",
    "            \n",
    "            if env.grid[environment.sample(s, action)] == \"W\":\n",
    "                continue\n",
    "            \n",
    "            if Heu.l2_norm(environment.state_to_pos(s), goalpos) > (1 + Heu.l2_norm(p1, goalpos)): # Controllo se l2_norm è consistente\n",
    "                return False\n",
    "    return True\n",
    "        \n",
    "        # You should return a boolean value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adb3a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You proved that the euristic is admissible.\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "admissible = check_euristic(env)\n",
    "if admissible is None:\n",
    "    print(\"Provide a boolean value that states the admissibility of the euristic!\")\n",
    "elif admissible:\n",
    "    print(\"You proved that the euristic is admissible.\\n\")\n",
    "elif not admissible:\n",
    "    print(\"You proved that the euristic is not admissible.\\n\")\n",
    "check_2_1(check_euristic(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dce6e",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2) Given the results of exercise 2.1, choose the version of A* that finds a minimum cost path from S (0,0) to G (7,7) guaranteeing optimality: [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34a71474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def present_with_higher_cost(queue, node):\n",
    "    if node.state in queue:\n",
    "        if queue[node.state].value > node.value: \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def a_star(environment): #feel free to add all the arguments you need!\n",
    "    \n",
    "    path = []\n",
    "    time_cost = 1\n",
    "    space_cost = 1\n",
    "    \n",
    "    '''\n",
    "        YOUR CODE HERE\n",
    "    '''\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    queue = PriorityQueue()\n",
    "    \n",
    "    queue.add(Node(environment.startstate))\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        if queue.is_empty(): \n",
    "            return None, time_cost, space_cost\n",
    "        \n",
    "        # Retrieve node from the queue\n",
    "        node = queue.remove()\n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        # Look around\n",
    "        for action in range(environment.action_space.n):\n",
    "            \n",
    "            p1 = environment.state_to_pos(environment.sample(node.state, action))\n",
    "            \n",
    "            # Child node where pathcost is the pathcost of parent + 1 and value is L2_norm\n",
    "            child = Node(environment.sample(node.state, action), node, node.pathcost + 1, Heu.l2_norm(p1, goalpos) + node.pathcost + 1)  \n",
    "            time_cost += 1\n",
    "            \n",
    "            queue.add(child)\n",
    "                \n",
    "            if present_with_higher_cost(queue, child):\n",
    "                queue.replace(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue))\n",
    "    \n",
    "    \n",
    "    \n",
    "    return path, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d53d9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.5323\n",
      "\n",
      "Chosen A* version:  Tree search\n",
      "Solution: [(1, 0), (2, 0), (2, 1), (3, 1), (3, 2), (3, 3), (3, 4), (4, 4), (4, 5), (5, 5), (6, 5), (7, 5), (7, 6), (7, 7)]\n",
      "N° of nodes explored: 6473\n",
      "Max n° of nodes in memory: 4855\n",
      "\n",
      "Environment: BigFrozenLakeEnv-v0\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "# Uncomment the version you want to use!\n",
    "# a_star_version = \"\"\n",
    "a_star_version = \"Tree search\"  \n",
    "# a_star_version = \"Graph search\"\n",
    "\n",
    "path, time_cost, memory_cost = a_star(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "print(\"Chosen A* version: \", a_star_version)\n",
    "print(\"Solution: {}\".format(solution_2_string(path, env)))\n",
    "print(\"N° of nodes explored: {}\".format(time_cost))\n",
    "print(\"Max n° of nodes in memory: {}\\n\".format(memory_cost))\n",
    "\n",
    "check_2_2(env_name, solution_2_string(path, env), time_cost, memory_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec74707",
   "metadata": {},
   "source": [
    "### 2.3) Now focus on the environment in figure:\n",
    "\n",
    "   <img src=\"images/frozen_lake_left.png\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "### Assume that the actions are again fully deterministic, and that moving left (action id: 0) doubles the cost of that move for the agent. Choose the best way to compute the optimal solution assuming that no heuristic is given. Motivate your choice.  [15 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b651249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'F' 'F' 'F']\n",
      " ['F' 'W' 'W' 'W' 'W' 'F']\n",
      " ['F' 'W' 'G' 'F' 'F' 'F']\n",
      " ['F' 'W' 'F' 'F' 'F' 'W']\n",
      " ['F' 'W' 'F' 'W' 'F' 'F']\n",
      " ['F' 'W' 'F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'F' 'F' 'W' 'F']]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'LeftFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb331cd",
   "metadata": {},
   "source": [
    "#### Pay attention: it is on you to calculate the cost of each action! Recall that the encoding for actions is the following: {0: 'L', 1: 'R', 2: 'U', 3: 'D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "24b34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(environment): #feel free to add all the arguments you need!\n",
    "    \n",
    "    path = []\n",
    "    time_cost = 1  # il costo iniziale è 1 e non 0!  \n",
    "    space_cost = 1  # il costo iniziale è 1 e non 0!\n",
    "    \n",
    "    '''\n",
    "        YOUR CODE HERE\n",
    "    '''\n",
    "    queue = PriorityQueue()\n",
    "    queue.add(Node(environment.startstate))\n",
    "    \n",
    "    explored = set()\n",
    "    \n",
    "    while True:\n",
    "        if queue.is_empty(): \n",
    "            return None\n",
    "        \n",
    "        # Retrieve node from the queue\n",
    "        node = queue.remove()  \n",
    "        if node.state == environment.goalstate: \n",
    "            return build_path(node), time_cost, space_cost\n",
    "        \n",
    "        explored.add(node.state)\n",
    "        \n",
    "        # Look around\n",
    "        for action in range(environment.action_space.n):\n",
    "            \n",
    "            if action == 0: # action è 'left', quindi raddoppio il costo -> +2\n",
    "                child = Node(environment.sample(node.state, action), node, node.pathcost + 2 , node.pathcost + 2)\n",
    "            else:\n",
    "                # Child node where value and pathcost are both the pathcost of parent + 1\n",
    "                child = Node(environment.sample(node.state, action), node, node.pathcost + 1, node.pathcost + 1)  \n",
    "            time_cost += 1\n",
    "            \n",
    "            if child.state not in queue and child.state not in explored:\n",
    "                queue.add(child)\n",
    "                \n",
    "            elif present_with_higher_cost(queue, child):\n",
    "                queue.replace(child)\n",
    "                \n",
    "        space_cost = max(space_cost, len(queue) + len(explored))\n",
    "    \n",
    "    return path, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09286e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0048\n",
      "\n",
      "Search strategy:  UCS\n",
      "Your solution:  [(1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (6, 1), (6, 2), (5, 2), (4, 2), (3, 2), (2, 2)]\n",
      "Time cost:  113\n",
      "Space cost:  31\n",
      "Environment: LeftFrozenLakeEnv-v0\n",
      "\n",
      "\u001b[1m\u001b[92mYour solution is correct!\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "path, time_cost, space_cost = solution(env)\n",
    "strategy = 'UCS' #indicate the search strategy you chose to use\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "print(\"Search strategy: \", strategy)\n",
    "print(\"Your solution: \", solution_2_string(path, env))\n",
    "print(\"Time cost: \", time_cost)\n",
    "print(\"Space cost: \", space_cost)\n",
    "\n",
    "check_2_3(env_name, solution_2_string(path, env), time_cost, space_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3507ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
