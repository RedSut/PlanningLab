{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# Planning-Lab Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/frozen_lake.gif\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to cross a frozen lake to reach the gift in cell $(3,3)$ without falling into any Holes(H) by walking over the Frozen(F) lake. In addition to avoid falling into water holes (cells $(1,1)$, $(1,3)$, $(2,3)$ and $(3,0)$), the agent has to reach its destination as fast as possible, since walking on the frozen surface hurts him.\n",
    "\n",
    "However, the agent may not always move in the chosen direction due to the slippery nature of the frozen lake! In fact, he will move in the chosen direction with probability of 1/2 else it will move in the perpendicular directions with equal probability of 1/4 for each direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'F']\n",
      " ['F' 'H' 'F' 'H']\n",
      " ['F' 'F' 'F' 'H']\n",
      " ['H' 'F' 'F' 'G']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of start state:  S\n",
      "Cell type of cell 7:  H\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "# ESEMPI ENV.GRID\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "from utils.exam_checker import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'SlipperyFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 7 # a generic state of the environment,\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(f\"Cell type of cell {state}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc195b2",
   "metadata": {},
   "source": [
    "### 1.1) Given the environment reported above, and assuming that the discount factor is set to 0.9, find the optimal way for the agent to reach the treasure and print the solution using the provided code: [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15e2e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_solution(environment): #feel free to add all the arguments you need!\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # you should return the policy as a 1-d array of action identifiers, where the ùëñ-th action refers to the ùëñ-th state:\n",
    "    return np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8beb9acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0001\n",
      "\n",
      "Environment: SlipperyFrozenLakeEnv-v0\n",
      "\n",
      "Your solution:\n",
      "[['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']]\n",
      "\n",
      "\u001b[91mYour solution is not correct\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "solution = your_solution(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "# this function converts your solution to the corresponding 2-d action matrix:\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "check_1_1(env_name, solution_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b715f9",
   "metadata": {},
   "source": [
    "### 1.2) Now focus on state (2,1) and assume the value funcion is given (see code). Discuss which action is the optimal one for that specific state and why. Use code to motivate your answer: [15 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf924170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct action to perform should be: None\n",
      "\u001b[91mYour solution is not correct\n"
     ]
    }
   ],
   "source": [
    "actions = {0: \"L\", 1: \"R\", 2: \"U\", 3: \"D\"}\n",
    "\n",
    "value_function = [-0.33, -0.36, -0.38, -0.38, -0.3, -1, -0, -1, -0.22, 0.02, -0.03, -1, -1, 0.23, 0.52, 1]\n",
    "id_start_state = 9\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "values_ex = [0, 0, 0, 0]\n",
    "\n",
    "'''\n",
    "YOUR CODE HERE\n",
    "'''\n",
    "\n",
    "correct_action = None\n",
    "print(f'The correct action to perform should be: {correct_action}')\n",
    "check_2_1(correct_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d79d8",
   "metadata": {},
   "source": [
    "### 1.3) Now suppose that we want the agent to avoid holes at all costs, as shown in the policy below:\n",
    "<img src=\"images/policy.png\"/>\n",
    "\n",
    "### Change the reward array in order to force the agent to behave in this specific way. [15 Points]\n",
    "(the start and goal positions are the same as in exercise 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71056459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: ModifiableFrozenLakeEnv-v0\n",
      "\n",
      "Your solution:\n",
      "[['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']\n",
      " ['L' 'L' 'L' 'L']]\n",
      "\n",
      "\u001b[91mYour solution is not correct\n"
     ]
    }
   ],
   "source": [
    "env_name = 'ModifiableFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Change the reward specification above, the reward array will be updated by a dedicated funcion:\n",
    "rewards = {\"F\": -0.04, \"S\": -0.04, \"H\": -0.2, \"G\": 1} \n",
    "env.RS = update_reward_array(rewards)\n",
    "\n",
    "solution = your_solution(env) #Use your solution from exercise 1.1 to test your changes\n",
    "solution_render = np.vectorize(env.actions.get)(solution.reshape(env.rows, env.cols)) \n",
    "check_1_3(env_name, solution_render)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f319b3",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce193bb",
   "metadata": {},
   "source": [
    "Now consider the following environment:\n",
    "    <img src=\"images/frozen_lake_big.png\" style=\"zoom: 30%;\"/>\n",
    "    \n",
    "The lake is much bigger, but the actions are no longer stochastic. **S=(0,0)** and **G=(7,7)** are the starting and goal positions respectively. Consider the problem of finding a minimum cost path from S to G assuming the agent can move in the four directions (except when a wall is present) and that each movement has a unitary cost. Answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038b0cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'W' 'F' 'F' 'F' 'F' 'W' 'F']\n",
      " ['F' 'W' 'F' 'W' 'F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'W' 'W' 'F' 'F' 'F' 'F']\n",
      " ['W' 'F' 'F' 'F' 'F' 'W' 'F' 'F']\n",
      " ['F' 'F' 'F' 'W' 'F' 'F' 'F' 'W']\n",
      " ['F' 'W' 'W' 'F' 'W' 'F' 'W' 'F']\n",
      " ['F' 'W' 'F' 'F' 'W' 'F' 'W' 'F']\n",
      " ['F' 'F' 'F' 'W' 'F' 'F' 'F' 'G']]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'BigFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "env.render() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61812b97",
   "metadata": {},
   "source": [
    "### 2.1) First of all, verify whether the Euclidean distance (l2_norm) is an  *admissible* heuristic in this environment. In particular, you should implement a function that checks whether for every state, the admissibility condition is verified. The function should return true if the heuristic is consistent and false otherwise. Keep in mind that every action has unitary cost. [15 Points]\n",
    "\n",
    "Hint: The function below can be used as an oracle to know the real cost of going from a node n to the goal state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fe8e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost from start to goal:  14\n",
      "Cost from position (3,3) to goal:  8\n"
     ]
    }
   ],
   "source": [
    "print(\"Cost from start to goal: \", real_cost_to_goal(env.startstate))\n",
    "print(\"Cost from position (3,3) to goal: \", real_cost_to_goal(env.pos_to_state(3,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0917d6",
   "metadata": {},
   "source": [
    "#### From now on consider hole cells as unaccessible for the agent (they are in fact referred to as walls, \"W\", in the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "585957d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_euristic(environment):\n",
    "    goalpos = environment.state_to_pos(environment.goalstate)\n",
    "    for s in range(environment.observation_space.n): \n",
    "        if env.grid[s] == \"W\":\n",
    "            continue\n",
    "        '''\n",
    "        YOUR CODE HERE\n",
    "        '''\n",
    "        # You should return a boolean value!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adb3a820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provide a boolean value that states the admissibility of the euristic!\n",
      "\u001b[91mYour solution is not correct\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(env_name)\n",
    "admissible = check_euristic(env)\n",
    "if admissible is None:\n",
    "    print(\"Provide a boolean value that states the admissibility of the euristic!\")\n",
    "elif admissible:\n",
    "    print(\"You proved that the euristic is admissible.\\n\")\n",
    "elif not admissible:\n",
    "    print(\"You proved that the euristic is not admissible.\\n\")\n",
    "check_2_1(check_euristic(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7dce6e",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2) Given the results of exercise 2.1, choose the version of A* that finds a minimum cost path from S (0,0) to G (7,7) guaranteeing optimality: [20 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34a71474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_star(environment): #feel free to add all the arguments you need!\n",
    "    \n",
    "    path = []\n",
    "    time_cost = 0\n",
    "    space_cost = 0\n",
    "    \n",
    "    '''\n",
    "        YOUR CODE HERE\n",
    "    '''\n",
    "    \n",
    "    return path, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d53d9074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0001\n",
      "\n",
      "Chosen A* version:  \n",
      "Solution: []\n",
      "N¬∞ of nodes explored: 0\n",
      "Max n¬∞ of nodes in memory: 0\n",
      "\n",
      "Environment: BigFrozenLakeEnv-v0\n",
      "\n",
      "\u001b[91mYour solution is not correct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "# Uncomment the version you want to use!\n",
    "a_star_version = \"\"\n",
    "# a_star_version = \"Tree search\"  \n",
    "# a_star_version = \"Graph search\"\n",
    "\n",
    "path, time_cost, memory_cost = a_star(env)\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "print(\"Chosen A* version: \", a_star_version)\n",
    "print(\"Solution: {}\".format(solution_2_string(path, env)))\n",
    "print(\"N¬∞ of nodes explored: {}\".format(time_cost))\n",
    "print(\"Max n¬∞ of nodes in memory: {}\\n\".format(memory_cost))\n",
    "\n",
    "check_2_2(env_name, solution_2_string(path, env), time_cost, memory_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec74707",
   "metadata": {},
   "source": [
    "### 2.3) Now focus on the environment in figure:\n",
    "\n",
    "   <img src=\"images/frozen_lake_left.png\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "### Assume that the actions are again fully deterministic, and that moving left (action id: 0) doubles the cost of that move for the agent. Choose the best way to compute the optimal solution assuming that no heuristic is given. Motivate your choice.  [15 Points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b651249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'F' 'F' 'F' 'F' 'F']\n",
      " ['F' 'W' 'W' 'W' 'W' 'F']\n",
      " ['F' 'W' 'G' 'F' 'F' 'F']\n",
      " ['F' 'W' 'F' 'F' 'F' 'W']\n",
      " ['F' 'W' 'F' 'W' 'F' 'F']\n",
      " ['F' 'W' 'F' 'F' 'F' 'F']\n",
      " ['F' 'F' 'F' 'F' 'W' 'F']]\n"
     ]
    }
   ],
   "source": [
    "env_name = 'LeftFrozenLakeEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb331cd",
   "metadata": {},
   "source": [
    "#### Pay attention: it is on you to calculate the cost of each action! Recall that the encoding for actions is the following: {0: 'L', 1: 'R', 2: 'U', 3: 'D'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24b34ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solution(environment): #feel free to add all the arguments you need!\n",
    "    \n",
    "    path = []\n",
    "    time_cost = 0\n",
    "    space_cost = 0\n",
    "    \n",
    "    '''\n",
    "        YOUR CODE HERE\n",
    "    '''\n",
    "    \n",
    "    return path, time_cost, space_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09286e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EXECUTION TIME: \n",
      "0.0001\n",
      "\n",
      "Search strategy:  \n",
      "Your solution:  []\n",
      "Time cost:  0\n",
      "Space cost:  0\n",
      "Environment: LeftFrozenLakeEnv-v0\n",
      "\n",
      "\u001b[91mYour solution is not correct\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "t = timer()\n",
    "\n",
    "path, time_cost, space_cost = solution(env)\n",
    "strategy = '' #indicate the search strategy you chose to use\n",
    "\n",
    "print(\"\\nEXECUTION TIME: \\n{}\\n\".format(round(timer() - t, 4)))\n",
    "\n",
    "print(\"Search strategy: \", strategy)\n",
    "print(\"Your solution: \", solution_2_string(path, env))\n",
    "print(\"Time cost: \", time_cost)\n",
    "print(\"Space cost: \", space_cost)\n",
    "\n",
    "check_2_3(env_name, solution_2_string(path, env), time_cost, space_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
